
import torch
import torch.nn as nn

from attention import(
    GroupedQueryAttention,
    compute_rope_params,
)


LLAMA32_CONFIG_1B = {
    "vocab_size": 128_256,           # Vocabulary size
    "context_length": 131_072,       # Context length that was used to train the model
    "emb_dim": 2048,                 # Embedding dimension
    "n_heads": 32,                   # Number of attention heads
    "n_layers": 16,                  # Number of layers
    "hidden_dim": 8192,              # Size of the intermediate dimension in FeedForward
    "n_kv_groups": 8,                # Key-Value groups for grouped-query attention
    "rope_base": 500_000.0,          # The base in RoPE's "theta"
    "dtype": torch.float32,

    "use_norm": True,
    "dropout": 0.1,
    "return_attn": True,

    # "dtype": torch.bfloat16,         # Lower-precision dtype to reduce memory usage
    "rope_freq": {                   # RoPE frequency scaling
        "factor": 32.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_context_length": 8192,
    }
}

LLAMA32_CONFIG_3B = {
    "vocab_size": 128_256,           # Vocabulary size
    "context_length": 131_072,       # Context length that was used to train the model
    "emb_dim": 3072,                 # Embedding dimension
    "n_heads": 24,                   # Number of attention heads
    "n_layers": 28,                  # Number of layers
    "hidden_dim": 8192,              # Size of the intermediate dimension in FeedForward
    "n_kv_groups": 8,                # Key-Value groups for grouped-query attention
    "rope_base": 500_000.0,          # The base in RoPE's "theta"
    "dtype": torch.bfloat16,         # Lower-precision dtype to reduce memory usage
    "rope_freq": {                   # RoPE frequency scaling
        "factor": 32.0,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
        "original_context_length": 8192,
    }
}


class QuirkBox(nn.Module):
    def __init__(self, cfg):
        super().__init__()

        # Main model parameters
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"], dtype=cfg["dtype"])

        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`
            [TransformerBlock(cfg) for _ in range(cfg["n_layers"])]
        )

        self.final_norm = nn.RMSNorm(cfg["emb_dim"], eps=1e-5, dtype=cfg["dtype"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False, dtype=cfg["dtype"])

        # Reusuable utilities
        cos, sin = compute_rope_params(
            head_dim=cfg["emb_dim"] // cfg["n_heads"],
            theta_base=cfg["rope_base"],
            context_length=cfg["context_length"],
            freq_config=cfg["rope_freq"]
        )
        self.register_buffer("cos", cos, persistent=False)
        self.register_buffer("sin", sin, persistent=False)
        self.cfg = cfg

    # ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å‹çš„å‰å‘ä¼ æ’­ forward() å‡½æ•°,åŒ…å«è¾“å…¥åµŒå…¥ã€ä½ç½®ç¼–ç ã€æ³¨æ„åŠ› maskã€å¤šå±‚ TransformerBlock çš„å †å ï¼Œä»¥åŠæœ€ç»ˆè¾“å‡º logits
    # è¾“å…¥ token id â†’ è¯åµŒå…¥ â†’ åŠ ä½ç½®ç¼–ç  â†’ é€šè¿‡å¤šå±‚ Transformer Block â†’ è¾“å‡ºé¢„æµ‹ logitsï¼ˆ+ å¯é€‰æ³¨æ„åŠ›çŸ©é˜µï¼‰
    def forward(self, in_idx, return_attn=False):    # in_idx: è¾“å…¥çš„ token ç´¢å¼•ï¼ˆå½¢çŠ¶å¯èƒ½æ˜¯ [batch_size, seq_len]ï¼‰
        tok_embeds = self.tok_emb(in_idx)   # æŠŠ token id è½¬æ¢æˆ è¯å‘é‡åµŒå…¥ï¼Œå½¢çŠ¶å˜ä¸º [batch, seq_len, emb_dim], self.tok_emb æ˜¯åµŒå…¥å±‚ï¼ˆnn.Embeddingï¼‰
        x = tok_embeds

        # æ„å»ºä¸€ä¸ª ä¸Šä¸‰è§’çš„æ³¨æ„åŠ› maskï¼Œç”¨äº å› æœæ³¨æ„åŠ›ï¼ˆcausal attentionï¼‰ï¼šé˜²æ­¢æ¯ä¸ª token çœ‹åˆ°æœªæ¥çš„ token
        num_tokens = x.shape[1]
        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)

        attn_maps = []

        for block in self.trf_blocks:   # self.trf_blocks æ˜¯å¤šä¸ªå †å çš„ TransformerBlockï¼ˆé€šå¸¸æ˜¯ nn.ModuleListï¼‰ã€‚å¯¹è¾“å…¥ä¾æ¬¡åº”ç”¨å¤šä¸ª Blockï¼Œè¿›è¡Œæ·±å±‚ç‰¹å¾æå–
            if return_attn:
                x, attn = block(x, mask, self.cos, self.sin, return_attn = True)
                attn_maps.append(attn)
            else:
                x = block(x, mask, self.cos, self.sin)

        x = self.final_norm(x)  # å¯¹æœ€åè¾“å‡ºåšä¸€æ¬¡å½’ä¸€åŒ–ï¼ˆRMSNorm æˆ– LayerNormï¼‰
        logits = self.out_head(x.to(self.cfg["dtype"])) # æŠŠ transformer çš„è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨å¤§å°çš„ç»´åº¦ï¼Œç”¨äºè®¡ç®—æ¦‚ç‡ï¼›out_head æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆç±»ä¼¼ nn.Linear(emb_dim, vocab_size)ï¼‰ï¼›.to(self.cfg["dtype"]) å¯èƒ½æ˜¯ä¸ºäº†ç»Ÿä¸€ç²¾åº¦ï¼ˆæ¯”å¦‚ float16 æˆ– float32ï¼‰
        return (logits, attn_maps) if return_attn else logits


class TransformerBlock(nn.Module):  # ç»§æ‰¿è‡ª nn.Moduleï¼Œè¡¨æ˜å®ƒæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å—
    def __init__(self, cfg):
        super().__init__()  # æ¥æ”¶ä¸€ä¸ª cfg é…ç½®å­—å…¸ï¼Œæ§åˆ¶è¶…å‚æ•°å’ŒåŠŸèƒ½å¯ç”¨
        self.use_norm = cfg.get("use_norm", True)   # é»˜è®¤å¯ç”¨å½’ä¸€åŒ–
        self.dropout = nn.Dropout(cfg.get("dropout", 0.1))  # é˜²æ­¢è¿‡æ‹Ÿåˆçš„ dropout å±‚ï¼Œé»˜è®¤ 0.1
        # å®ä¾‹åŒ–ä¸€ä¸ª åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å±‚ï¼ˆGroupedQueryAttentionï¼‰
        self.att = GroupedQueryAttention(   
            d_in=cfg["emb_dim"],    # å‚æ•°å¦‚åµŒå…¥ç»´åº¦ã€å¤´æ•°ã€é”®å€¼åˆ†ç»„æ•°ã€æ•°æ®ç±»å‹éƒ½æ¥è‡ª cfg
            d_out=cfg["emb_dim"],
            num_heads=cfg["n_heads"],
            num_kv_groups=cfg["n_kv_groups"],
            dtype=cfg["dtype"]
        )
        self.ff = FeedForward(cfg)  # ä¸€ä¸ªæ ‡å‡†çš„ä¸¤å±‚å‰é¦ˆç¥ç»ç½‘ç»œæ¨¡å—
        if self.use_norm:   # å¦‚æœå¯ç”¨å½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨ RMSNorm æ›¿ä»£å¸¸è§„çš„ LayerNorm
            self.norm1 = nn.RMSNorm(cfg["emb_dim"], eps=1e-5, dtype=cfg["dtype"])
            self.norm2 = nn.RMSNorm(cfg["emb_dim"], eps=1e-5, dtype=cfg["dtype"])

    def forward(self, x, mask, cos, sin, return_attn=False):   # x: è¾“å…¥åµŒå…¥ (å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, seq_len, emb_dim])ï¼›mask: æ³¨æ„åŠ› maskï¼Œç”¨äºå±è”½ä¸ç›¸å…³ä½ç½®ï¼›cos, sin: æ—‹è½¬ä½ç½®ç¼–ç å‚æ•°ï¼ˆå¯èƒ½ç”¨äº RoPEï¼‰ï¼›return_attn: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        print("ğŸ¥°ğŸ’•ğŸ’–"*10)#("ğŸ§ª GQA forward(), return_attn =", return_attn)
        # ğŸ”· ç¬¬ä¸€æ®µï¼šå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼ˆå¤„ç† token é—´ä¾èµ–ï¼‰
        # Pre-Norm Transformer: Shortcut connection for attention block
        # å½’ä¸€åŒ– â†’ å­æ¨¡å—ï¼ˆAttention æˆ– FFï¼‰â†’ Dropout â†’ æ®‹å·®è¿æ¥
        shortcut = x    # ä¿å­˜å½“å‰çš„è¾“å…¥ xï¼Œè¿™æ˜¯ä¸ºäº† ç¨åç”¨äºæ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰
        if self.use_norm:
            x = self.norm1(x)   
        x, attn_weights = self.att(x, mask, cos, sin, return_attn=return_attn)   # ä¼ å…¥æ³¨æ„åŠ›å±‚ï¼Œè·å¾—æ–°çš„è¡¨ç¤º x å’Œï¼ˆå¯é€‰çš„ï¼‰æ³¨æ„åŠ›æƒé‡
        #x = self.att(x, mask, cos, sin)  # Shape [batch_size, num_tokens, emb_size]
        x = self.dropout(x)     # åœ¨è¾“å‡ºååº”ç”¨ Dropoutï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚Dropout æ˜¯è®­ç»ƒæ—¶â€œéšæœºä¸¢å¼ƒç¥ç»å…ƒâ€çš„æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
        x = x + shortcut  # Add the original input back # æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ã€‚å°†å‰é¦ˆå±‚çš„è¾“å‡ºåŠ ä¸ŠåŸå§‹è¾“å…¥ xï¼ˆä¿å­˜åœ¨ shortcut ä¸­ï¼‰ã€‚è¿™æ ·åšå¯ä»¥è®©æ¢¯åº¦æ›´å®¹æ˜“åå‘ä¼ æ’­ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

        # ğŸ”· ç¬¬äºŒæ®µï¼šå‰é¦ˆç¥ç»ç½‘ç»œ FeedForwardï¼ˆFFNï¼‰ï¼ˆå¤„ç†æ¯ä¸ª token çš„è¡¨ç¤ºï¼‰
        # Shortcut connection for feed-forward block
        shortcut = x
        if self.use_norm:
            x = self.norm2(x) # å¯¹è¾“å…¥ x åšç¬¬äºŒæ¬¡å½’ä¸€åŒ–ï¼ˆè¿™é‡Œä½¿ç”¨çš„æ˜¯ RMSNormï¼‰ã€‚è¿™æ˜¯æ ‡å‡† Transformer çš„å‰é¦ˆå±‚å‰ä½¿ç”¨å½’ä¸€åŒ–çš„æ–¹å¼ï¼ˆä¹Ÿå°±æ˜¯ Pre-Normï¼‰ã€‚
        x = self.ff(x)  # é€šè¿‡å‰é¦ˆç½‘ç»œå¤„ç†è¾“å…¥
        x=self.dropout(x)
        x = x + shortcut  # Add the original input back

        return x if not return_attn else (x, attn_weights)  # å¦‚æœ return_attn=Trueï¼Œå°±è¿”å›è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡


class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.fc1 = nn.Linear(cfg["emb_dim"], cfg["hidden_dim"], dtype=cfg["dtype"], bias=False)
        self.fc2 = nn.Linear(cfg["emb_dim"], cfg["hidden_dim"], dtype=cfg["dtype"], bias=False)
        self.fc3 = nn.Linear(cfg["hidden_dim"], cfg["emb_dim"], dtype=cfg["dtype"], bias=False)

    def forward(self, x):
        x_fc1 = self.fc1(x)
        x_fc2 = self.fc2(x)
        x = nn.functional.silu(x_fc1) * x_fc2
        return self.fc3(x)



# def text_to_token_ids(text, tokenizer):
#     encoded = tokenizer.encode(text)
#     encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
#     return encoded_tensor
def text_to_token_ids(text, tokenizer, allowed_special=set()):
    if hasattr(tokenizer, 'encode'):
        return torch.tensor(tokenizer.encode(text, allowed_special=allowed_special), dtype=torch.long)[None, :]
    else:
        raise ValueError("Tokenizer does not support .encode()")


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())


def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None, return_attn=False):

    attn_maps = []
    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        print("ğŸ§ª Step A: start generate()")
        idx_cond = idx[:, -context_size:]
        print("ğŸ§ª Step B: shape", idx_cond.shape)

        with torch.no_grad():
            if return_attn:
                logits, attn_maps = model(idx_cond, return_attn=True)
            else:
                logits = model(idx_cond)
        print("ğŸ§ª Step C: logits done", logits.shape)

        logits = logits[:, -1, :]

        # Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    if return_attn:
        print("ğŸ” Attention maps collected:", len(attn_maps))
        print("ğŸ” Layer 0 shape:", attn_maps[0].shape)  # (batch, heads, tokens, tokens)
        return (idx, attn_maps)
    else:
        return idx