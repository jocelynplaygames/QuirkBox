# ğŸ§  QuirkBox: A Lightweight, Visualizable Transformer Text Generation Platform

> âš¡ï¸ Extended from [rasbt/llama-3.2-from-scratch](https://github.com/rasbt/llama-3.2-from-scratch)
> ğŸ› ï¸ With enhancements for attention visualization, web UI, and configurable training.

---

## ğŸš€ Features

- âœ… Fully customizable Transformer model (`MiniGenieTransformer`)
- âœ… Switchable modules: LayerNorm, Dropout, Rotary Embeddings, Position Embeddings
- âœ… Exports attention weights for inspection and visualization
- âœ… Web UI with prompt-style selection & live generation
- âœ… Support for token embedding configuration
- âœ… Training-ready via Hydra config + Weights & Biases logging
- âœ… CLI + Web dual inference interface

---

## ğŸ“¦ Installation

```bash

git clone https://github.com/yourusername/QuirkBox.git
cd QuirkBox
pip install -r requirements.txt

```

Make sure you also have:

```bash

pip install torch tiktoken blobfile hydra-core wandb

```

---

## ğŸ“ Project Structure

```
QuirkBox/
â”œâ”€â”€ model.py               # ğŸ§  Redesigned MiniGenieTransformer (based on LLaMA3)
â”œâ”€â”€ generate.py            # ğŸ”® CLI generation with attention export
â”œâ”€â”€ app.py                 # ğŸŒ Flask Web API for interaction
â”œâ”€â”€ templates/index.html   # ğŸ¨ Frontend UI + canvas attention viewer
â”œâ”€â”€ static/attn.json       # ğŸ§ª Saved attention weights for visualization
â”œâ”€â”€ configs/               # âš™ï¸ Training config (Hydra)
â”œâ”€â”€ tokenizer.py           # ğŸ”¤ Tokenizer setup (from original project)
â”œâ”€â”€ weights/               # ğŸ“¦ Downloaded model weights (.pth)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

```

---

## ğŸ’» Usage: CLI Mode

Run inference from CLI:

```bash
python generate.py --input "Why do cats meow?" --style casual

```

Options:

- `-style`:
    - `casual`, `joke`, `sci-fi`, `poetry`, `therapy`
- `-output-attn`: export attention weights to `attn.json` for frontend visualization

---

## ğŸŒ Usage: Web UI

Launch the web interface:

```bash
python app.py

```

In browser:

- Select prompt style
- Enter message
- View response + attention map (layer/head selectable)

---

## âš™ï¸ Training & Experimentation

This repo includes **Hydra config + wandb logging** to simulate training:

```bash
python train.py experiment=demo

```

Outputs:

- Model checkpoints
- `wandb` metrics
- Configurable architecture (LayerNorm, Dropout, Embedding, etc.)

---

## ğŸ¯ What Makes This Project Unique?

| Feature | Description |
| --- | --- |
| âœ… Attention Export | Returns attention matrix from each layer/head |
| âœ… Visual Debugging | Canvas UI lets user view where model is focusing |
| âœ… Model Architecture | LayerNorm, Dropout, PosEmbed fully configurable |
| âœ… Modularity | Easy to add new attention types or layers |
| âœ… Training Ready | Configurable & extensible for small-scale training |

---

## Credits & Attribution

This project builds upon:

- [rasbt/llama-3.2-from-scratch](https://github.com/rasbt/llama-3.2-from-scratch)
    
    A minimal, educational PyTorch implementation of LLaMA 3.2
