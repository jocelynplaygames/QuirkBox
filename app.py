


from flask import Flask, render_template, request, jsonify
import torch
from model import QuirkBox, generate, text_to_token_ids, token_ids_to_text
from tokenizer import Llama3Tokenizer, ChatFormat, clean_text
from prompt_templates import TEMPLATES
from config.app_config import MODEL_FILE, TOKENIZER_FILE, DEVICE
from generate import run_generation
import json

MODEL_FILE = "llama3.2-1B-instruct.pth"
TOKENIZER_FILE = "tokenizer.model"
MODEL_CONTEXT_LENGTH = 256
MAX_NEW_TOKENS = 10 #æ¨¡å‹æ¯æ¬¡ç”Ÿæˆæ—¶ æœ€å¤šè¾“å‡ºå‡ ä¸ªè¯ï¼ˆtokenï¼‰.æ„æ€æ˜¯ â€”â€” è®©æ¨¡å‹ã€Œç”Ÿæˆ 10 ä¸ªæ–°çš„ tokenã€ã€‚é‚£æ•´ä¸ª generate() å‡½æ•°å†…éƒ¨çš„é€»è¾‘å°±ä¼šé‡å¤æ‰§è¡Œ 10 æ¬¡ã€‚
TEMPERATURE = 0.0
TOP_K = 1

app = Flask(__name__)   # è¿è¡Œçš„ Flask æœåŠ¡
app.config['MAX_CONTENT_LENGTH'] = 1 * 1024 * 1024  # é™åˆ¶æœ€å¤§è¯·æ±‚ä½“ä¸º 1MB


# ä¸‹è½½æƒé‡ & tokenizer
import os, urllib.request

if not os.path.exists(MODEL_FILE):
    print("Downloading model...")
    urllib.request.urlretrieve(
        f"https://huggingface.co/rasbt/llama-3.2-from-scratch/resolve/main/{MODEL_FILE}",
        MODEL_FILE,
    )

if not os.path.exists(TOKENIZER_FILE):
    print("Downloading tokenizer...")
    urllib.request.urlretrieve(
        f"https://huggingface.co/rasbt/llama-3.2-from-scratch/resolve/main/{TOKENIZER_FILE}",
        TOKENIZER_FILE,
    )

# é…ç½®
from model import LLAMA32_CONFIG_1B as LLAMA32_CONFIG
LLAMA32_CONFIG["context_length"] = MODEL_CONTEXT_LENGTH

# åˆå§‹åŒ–æ¨¡å‹
model = QuirkBox(LLAMA32_CONFIG)
model.load_state_dict(torch.load(MODEL_FILE, map_location="cpu", weights_only=True))
model.eval()
model.to("cpu")

# åˆå§‹åŒ– tokenizer
tokenizer = ChatFormat(Llama3Tokenizer(TOKENIZER_FILE))


@app.route("/")
def index():
    return render_template("index.html")

@app.route('/favicon.ico')
def favicon():
    return '', 204  # æ·»åŠ  favicon è·¯ç”±ï¼Œé¿å…æµè§ˆå™¨è‡ªåŠ¨è¯·æ±‚è¿™ä¸ªå¯¼è‡´å´©ï¼›204 è¡¨ç¤ºâ€œæ— å†…å®¹â€ï¼Œæµè§ˆå™¨å°±ä¸ä¼šå†è¯·æ±‚äº†


@app.route("/generate", methods=["POST"])
def generate_text():
    data = request.json
    user_input = data.get("input", "")
    style = data.get("style", "casual")

    try:
        print("âœ… Step 1: Received request")

        prompt = TEMPLATES.get(style, "Talk to me about:") + "\n" + user_input
        print("âœ… Step 2: Built prompt:", prompt)

        with torch.no_grad():
            input_ids = text_to_token_ids(prompt, tokenizer).to("cpu")
            print("âœ… Step 3: Encoded input. Shape:", input_ids.shape)

            output_ids, attn_maps = generate(
                model=model,
                idx=input_ids,
                max_new_tokens=MAX_NEW_TOKENS,
                context_size=LLAMA32_CONFIG["context_length"],
                top_k=TOP_K,
                temperature=TEMPERATURE,
                return_attn=True
            )
            print("âœ… Step 4: Generation done. Shape:", output_ids.shape)

            output_text = token_ids_to_text(output_ids, tokenizer)
            print("âœ… Step 5: Decoded output text:", output_text)

            output_text = clean_text(output_text)
            print("âœ… Step 6: Cleaned output")
        
        #ä¿å­˜attention JSON åˆ° static/attn,json
        attn_serialized = [
            [head.cpu().tolist() for head in layer] for layer in attn_maps
        ]
        with open("static/attn.json","w") as f:
            json.dump(attn_serialized, f)
        print("âœ… Step 7: Saved attention map to static/attn.json")

        return jsonify({"output": output_text})
    # try:
    #     prompt = TEMPLATES.get(style, "Talk to me about:") + "\n" + user_input
    #     print("ğŸ§ª Prompt:", prompt)

    #     # mock éƒ¨åˆ†ï¼Œç”¨äºæ’æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹é˜»å¡
    #     output_text = f"Echo: {prompt}"  # âœ… ä¸´æ—¶æ›¿ä»£çœŸæ­£çš„ç”Ÿæˆ
    #     return jsonify({"output": output_text})
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"output": f"[ERROR] {str(e)}"})




if __name__ == "__main__":
    app.run(debug=True, use_reloader=False)
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000, debug=True)
#     #app.run(debug=True, use_reloader=False)